# -*- coding: utf-8 -*-
"""CIFAR10_Augmentation_RES34.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBT1B0CvWLRm-OVUaqh6Kv8V783wCfOX
"""

import os
import shutil
import random
import time
import sys
import warnings
warnings.filterwarnings("ignore")
import copy
import requests
from collections import Counter
import numpy as np
import pandas as pd
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.model_selection import train_test_split
from keras.datasets import mnist, fashion_mnist, cifar10, cifar100
from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, ConcatDataset
from torchvision import datasets, transforms, models
from torchvision.transforms import v2
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

# !pip install timm

def idle_wait(interval=60, num=1000):
    import datetime, pytz, time
    def get_current_timestr():
        tz = pytz.timezone('Asia/Shanghai')
        now = datetime.datetime.now(tz)
        dt_str = now.strftime("%Y-%m-%d %H:%M:%S")
        return dt_str

    for i in range(num):
        print(f'i: {i:<5d}, current_time: {get_current_timestr()}')
        time.sleep(interval)

def seed_everything(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # some cudnn methods can be random even after fixing the seed
    # unless you tell it to be deterministic
    torch.backends.cudnn.deterministic = True

def shuffle_array(array_X, array_y, random_seed=2023):
    assert array_X.shape[0] == array_y.shape[0]
    idx_lst = [i for i in range(array_X.shape[0])]
    random.seed(random_seed)
    random.shuffle(idx_lst)
    array_X, array_y = array_X[idx_lst], array_y[idx_lst]
    return array_X, array_y

def show_data_characteristic(X, y, info_str=None):
    assert X.shape[0] == y.shape[0] and X.shape[0]%2 == 0
    X, y = X.copy().astype('float32'), y.copy().astype('float32')

    X1, X2 = X[0::2], X[1::2]
    X_mean1, X_std1 = X1.mean(), X1.std()
    X_mean2, X_std2 = X2.mean(), X2.std()
    X_diff_mean, X_diff_std = (X1-X2).mean(), (X1-X2).std()
    print(f'show_data_characteristic() {info_str:<5} X shape: {X.shape} {X_mean1:.9f} '
          f'{X_std1:.9f} {X_diff_mean:.9f} {X_diff_std:.9f}')

    y1, y2 = y[0::2], y[1::2]
    y_mean1, y_std1 = y1.mean(), y1.std()
    y_mean2, y_std2 = y2.mean(), y2.std()
    y_diff_mean, y_diff_std = (y1-y2).mean(), (y1-y2).std()
    print(f'show_data_characteristic() {info_str:<5} y shape: {y.shape} {y_mean1:.9f} '
          f'{y_std1:.9f} {y_diff_mean:.9f} {y_diff_std:.9f}')

def load_image(img_file_path):
    img_data = Image.open(img_file_path)
    img_data = np.array(img_data)
    return img_data

def show_image(img_data, img_title=None):
    plt.imshow(img_data)
    if img_title is not None and type(img_title)==str:
        plt.title(img_title)
    plt.axis('off')
    plt.show()

QUICK_CHECK = False
IMAGE_SIZE = 32 #224 #64
NUM_CLASSES = 100
# airplane, automobile (but not truck or pickup truck), bird, cat, deer, dog, frog, horse, ship, and truck (but not pickup truck)
CIFAR10_LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else 'cpu'
print(f'device: {device} current use device: {device_name}')

GLOBAL_SEED = 2023

global_start_t = time.time()
seed_everything(seed=42)
print('ok, get here')
# idle_wait(interval=600, num=1000)

# sys.exit(0)

seed_everything(seed=42)

(train_X, train_y), (test_X, test_y) = cifar100.load_data()
train_X, train_y = shuffle_array(train_X, train_y)
test_X, test_y = shuffle_array(test_X, test_y)
assert train_X.shape == (50000, 32, 32, 3)
assert test_X.shape == (10000, 32, 32, 3)
assert train_y.shape == (50000, 1)
assert test_y.shape == (10000, 1)
train_y, test_y = train_y.squeeze(), test_y.squeeze()

if QUICK_CHECK:
    train_X, train_y = train_X[:5000], train_y[:5000]
    test_X, test_y = test_X[:1000], test_y[:1000]

VALID_NUM = 500 if QUICK_CHECK else 5000
valid_X, valid_y = train_X[:VALID_NUM], train_y[:VALID_NUM]
train_X, train_y = train_X[VALID_NUM:], train_y[VALID_NUM:]

show_data_characteristic(train_X, train_y, 'train')
show_data_characteristic(valid_X, valid_y, 'valid')
show_data_characteristic(test_X, test_y, 'test')

RESERVED_NUM = 200 if QUICK_CHECK else 1000  # reserved for possible future other usage
reserved_X, reserved_y = train_X[:RESERVED_NUM], train_y[:RESERVED_NUM]
train_X, train_y  = train_X[RESERVED_NUM:], train_y[RESERVED_NUM:]

print('after split', train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape, test_y.shape)
train_data_lst, train_label_lst = list(train_X), list(train_y)
valid_data_lst, valid_label_lst = list(valid_X), list(valid_y)
test_data_lst, test_label_lst = list(test_X), list(test_y)

# 参考： https://github.com/kuangliu/pytorch-cifar/blob/master/main.py
train_transform = v2.Compose(
    [v2.ToTensor(),
     v2.RandomCrop(32, padding=4),
     #v2.Resize((int(1.08*IMAGE_SIZE), int(1.08*IMAGE_SIZE))),
     v2.RandomHorizontalFlip(),
     #v2.RandomCrop(size=[IMAGE_SIZE, IMAGE_SIZE]),
     #v2.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),  # from ImageNet
     v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

test_transform = v2.Compose(
    [v2.ToTensor(),
     v2.Resize((IMAGE_SIZE, IMAGE_SIZE)),
     #v2.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),  # from ImageNet
     v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

class MyCustomDatasetFromList(Dataset):
    def __init__(self, data_lst, label_lst, transform=None):
        assert type(data_lst)==list and type(label_lst)==list
        assert len(data_lst)==len(label_lst)
        self.data_lst = data_lst
        self.label_lst = label_lst
        self.transform = transform

    def __len__(self):
        return len(self.label_lst)

    def __getitem__(self, idx):
        img_data = self.data_lst[idx]
        label = self.label_lst[idx]
        if self.transform is None:
            img_data = cv2.resize(img_data, (IMAGE_SIZE, IMAGE_SIZE))
            img_data = img_data / 255.0
            img_data = torch.from_numpy(img_data.transpose([2, 0, 1]))
        else:
            img_data = self.transform(img_data)
        img_data = img_data.float()
        return img_data, label

dataset_train = MyCustomDatasetFromList(train_data_lst, train_label_lst, transform=train_transform)
dataset_valid = MyCustomDatasetFromList(valid_data_lst, valid_label_lst, transform=test_transform)
dataset_test = MyCustomDatasetFromList(test_data_lst, test_label_lst, transform=test_transform)

data_loader_train = DataLoader(dataset=dataset_train, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)
data_loader_valid = DataLoader(dataset=dataset_valid, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)
data_loader_test = DataLoader(dataset=dataset_test, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)

print(f'len(data_loader_train): {len(data_loader_train)}, len(data_loader_valid): {len(data_loader_valid)}, '
      f'len(data_loader_test): {len(data_loader_test)}')

class MyCNN(nn.Module):
    def __init__(self, out_classes=10):
        super().__init__()
        self.out_classes = out_classes
        self.conv1 = nn.Conv2d(3, 32, 5, stride=1, padding=2)
        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=1)
        self.conv4 = nn.Conv2d(128, 256, 3, stride=1, padding=1)
        self.m = nn.AdaptiveAvgPool2d(output_size=(4, 4))
        self.fc = nn.Sequential(
            nn.Linear(4*4*256, 1000),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(1000, 500),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(500, out_classes),
        )

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2, 2)
        x = F.relu(self.conv3(x))
        x = F.max_pool2d(F.relu(self.conv4(x)), 2, 2)
        x = self.m(x)
        x = x.reshape(x.shape[0], -1)
        #my_print('in forward() x.shape: ', x.shape)
        x = self.fc(x)
        return x

class MyMLP(nn.Module):
    def __init__(self, out_classes=10):
        super(MyMLP, self).__init__()
        self.out_classes = out_classes
        self.fc = nn.Sequential(
            nn.Linear(3*32*32, 5000),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(5000, 2500),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(2500, 1000),
            nn.ReLU(),
            nn.Dropout(p=0.3),
            nn.Linear(1000, out_classes),
        )

    def forward(self, x):
        x = x.reshape(x.shape[0], -1)
        x = self.fc(x)
        return x

def getPretrainedResnet18_old(pretrained=True, out_classes=10, change_conv1=True):
    model = models.resnet18(pretrained=pretrained)
    if change_conv1:
        #model.conv1 = nn.Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, out_classes)
    return model

def getPretrainedResnet18(pretrained=True, out_classes=10, change_conv1=True):
    model = models.resnet18(pretrained=pretrained)
    if change_conv1:
        #model.conv1 = nn.Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        model.maxpool = nn.Identity()
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, out_classes)
    return model

def getPretrainedResnet34(pretrained=True, out_classes=10, change_conv1=True):
    model = models.resnet34(pretrained=pretrained)
    if change_conv1:
        #model.conv1 = nn.Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, out_classes)
    return model

def getPretrainedResnet50(pretrained=True, out_classes=10):
    model = models.resnet50(pretrained=pretrained)
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, out_classes)
    return model

def getPretrainedVGG16(pretrained=True, out_classes=10):
    model = models.vgg16(pretrained=pretrained)
    features = list(model.features)
    features[4] = nn.Identity()
    features[9] = nn.Identity()
    features[16] = nn.Identity()
    model.features = nn.Sequential(*features)
    classifier = list(model.classifier)
    classifier[6] = nn.Linear(classifier[6].in_features, out_classes)
    model.classifier = nn.Sequential(*classifier)
    return model

def getPretrainedDeit(pretrained=True):
    model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=pretrained)
    num_features = model.head.in_features
    model.head = nn.Linear(num_features, 10)
    return model

def getPretrainedAlexNet(pretrained=True, out_classes=10):
    model = models.alexnet(pretrained=pretrained)

    features = list(model.features)
    features[0] = nn.Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
    #features[0] = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    model.features = nn.Sequential(*features)

    layers_lst = list(model.classifier)
    last_layer = nn.Linear(layers_lst[-1].in_features, out_classes)
    layers_lst = layers_lst[:-1] + [last_layer]
    model.classifier = nn.Sequential(*layers_lst)
    return model

def getResNet(num_classes=10, **kwargs):
    from torchvision.models.resnet import resnet18
    model = resnet18(num_classes=num_classes, **kwargs)
    return model

# https://stackoverflow.com/questions/58297197/how-to-change-activation-layer-in-pytorch-pretrained-module
# model = replace_layer(model, nn.ReLU, nn.Hardswish(True))
# model = replace_layer(model, nn.Hardswish, nn.Softplus())
def replace_layer(module: nn.Module, old: nn.Module, new: nn.Module, full_name="", print_info=False):
    for name, m in module.named_children():
        full_name = f"{full_name}.{name}"
        if isinstance(m, old):
            setattr(module, name, new)
            if print_info:
                print(f"replaced {full_name}: {old}->{new}")
        elif len(list(m.children())) > 0:
            replace_layer(m, old, new, full_name)
    return module


# model = MyCNN()
# model = getPretrainedResnet18(pretrained=False, out_classes=NUM_CLASSES, change_conv1=True)
model = getPretrainedResnet34(pretrained=False, out_classes=NUM_CLASSES, change_conv1=True)
# model = getPretrainedResnet50(pretrained=True)
# model = getAlreadyTrainedResnet18()
# model = getPretrainedDeit(pretrained=True)
# model = getPretrainedVGG16(pretrained=True, out_classes=NUM_CLASSES)
# model = getPretrainedAlexNet(pretrained=False, out_classes=10)

model_param_num = sum(p.numel() for p in model.parameters())
model_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f'model_param_num: {model_param_num}, model_trainable_param_num: {model_trainable_param_num}')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else 'cpu'
print('current use device: ', device_name)
print(model)

print_num = 0
def my_print(*args):
    global print_num
    if print_num < 10:
        print(*args)
        print_num += 1

def get_optimizer_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']

def change_optimizer_lr(optimizer, lr):
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

def train(model, device, train_loader, optimizer, epoch):
    global cutmix_or_mixup
    model.train()
    start_t = time.time()
    for idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        #my_print('data.shape', data.shape, 'target:', target.shape)
        optimizer.zero_grad()
        if USE_FP16:
            with autocast():
                pred = model(data)
                loss = F.cross_entropy(pred, target)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            pred = model(data)
            loss = F.cross_entropy(pred, target)
            loss.backward()
            optimizer.step()
        if QUICK_CHECK and idx>=5:
            break

@torch.no_grad()
def test(model, device, test_loader, epoch, optimizer=None, flag_str='test'):
    global global_valid_acc, global_valid_epoch, DUMPED_PATH
    model.eval()
    start_t = time.time()
    total_loss, correct = 0., 0.
    true_label_lst, pred_label_lst, is_trigger_lst = [], [], []
    for idx, (data, target) in enumerate(test_loader):
        data, target = data.to(device), target.to(device)
        if USE_FP16:
            with autocast():
                output = model(data)
                loss = F.cross_entropy(output, target, reduction='sum')
        else:
            output = model(data)
            loss = F.cross_entropy(output, target, reduction='sum')
        total_loss += loss.item()
        pred = output.argmax(dim=1)
        correct += pred.eq(target.view_as(pred)).sum().item()
        pred_label_lst += list(pred.detach().cpu().numpy())
        true_label_lst += list(target.detach().cpu().numpy())
    whole_acc = metrics.accuracy_score(y_true=true_label_lst, y_pred=pred_label_lst)
    print(f'{flag_str:<5} whole_acc: {whole_acc:.7f}')

    if global_valid_acc < whole_acc and flag_str=='valid':
        global_valid_acc = whole_acc
        global_valid_epoch = epoch
        if optimizer is not None:
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, DUMPED_PATH)
        else:
            torch.save({
                'model_state_dict': model.state_dict(),
            }, DUMPED_PATH)
        print(f'got new global_valid_acc: {global_valid_acc:.7f} !!!')

print('get here ok')

seed_everything(seed=42)
USE_FP16 = True #False

# model = getPretrainedVGG16(pretrained=True, out_classes=NUM_CLASSES).to(device)
# model = getPretrainedResnet18(pretrained=False, out_classes=NUM_CLASSES, change_conv1=True).to(device)
model = getPretrainedResnet34(pretrained=True, out_classes=NUM_CLASSES, change_conv1=True).to(device)
model_param_num = sum(p.numel() for p in model.parameters())
model_trainable_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f'model_param_num: {model_param_num}, model_trainable_param_num: {model_trainable_param_num}')

use_scheduler = True

global_valid_acc = 0.0
global_valid_epoch = 0
num_epochs = 5 if QUICK_CHECK else 200 #40 #20

DUMPED_PATH = './cifar100_best_model_resnet34_seed42.pth'
# lr =  3e-4
# optimizer = torch.optim.Adam(model.parameters(), lr=lr)
lr = 0.1 #0.1
optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)
if use_scheduler:
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=210)
    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [60, 120, 180], 1/3.0)

print(f'pre training, get into test() start !!!')
test(model, device, data_loader_valid, -1, flag_str='pre_train_valid')
test(model, device, data_loader_test, -1, flag_str='pre_train_test')
print(f'pre training, get into test() finished !!!')

train_start_t = time.time()
for epoch in range(num_epochs):
    epoch_start_t = time.time()
    train(model, device, data_loader_train, optimizer, epoch)
    test(model, device, data_loader_valid, epoch, optimizer, flag_str='valid')
    test(model, device, data_loader_test, epoch, optimizer, flag_str='test')
    current_lr = get_optimizer_lr(optimizer)
    print(f'EPOCH {epoch} lr: {current_lr:.8f} global_valid_acc: {global_valid_acc:.7f} '
          f'global_valid_epoch: {global_valid_epoch} cost time: {time.time()-epoch_start_t:.2f} sec')

    if use_scheduler:
        scheduler.step()
print(f'training finished, cost time: {time.time()-train_start_t:.2f} sec')

# model = getPretrainedVGG16(pretrained=True, out_classes=NUM_CLASSES).to(device)
# model = getPretrainedResnet18(pretrained=False, out_classes=NUM_CLASSES, change_conv1=True).to(device)
model = getPretrainedResnet34(pretrained=True, out_classes=NUM_CLASSES, change_conv1=True).to(device)
checkpoint = torch.load(DUMPED_PATH, map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
print(f'after training, get into test() start !!!')
test(model, device, data_loader_valid, -1, flag_str='after_train_valid')
test(model, device, data_loader_test, -1, flag_str='after_train_test')
print(f'after training, get into test() finished !!!')



# # model = getPretrainedVGG16(pretrained=True, out_classes=NUM_CLASSES).to(device)
# # model = getPretrainedResnet18(pretrained=False, out_classes=NUM_CLASSES, change_conv1=True).to(device)
# model = getPretrainedResnet34(pretrained=True, out_classes=NUM_CLASSES, change_conv1=True).to(device)
# checkpoint = torch.load(DUMPED_PATH, map_location=device)
# model.load_state_dict(checkpoint['model_state_dict'])
# model.eval()
# print(f'after training, get into test() start !!!')
# test(model, device, data_loader_valid, -1, flag_str='after_train_valid')
# test(model, device, data_loader_test, -1, flag_str='after_train_test')
# print(f'after training, get into test() finished !!!')

# Load the pre-trained model
model = getPretrainedResnet34(pretrained=False, out_classes=NUM_CLASSES, change_conv1=True).to(device)

# # Load the saved weights
# checkpoint_path = '/content/cifar100_best_model_resnet34_seed42.pth'
# checkpoint = torch.load(checkpoint_path, map_location=device)
# model.load_state_dict(checkpoint['model_state_dict'])
# model.eval()

# Replace 'your_image_path.jpg' with the actual path to your input image
image_path = '/content/Evercrisp_NYAS-Apples2.png'
# CIFAR100_LABELS = ['apple', 'aquarium_fish', 'baby', 'cat', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'computer', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']

# Load and preprocess the image
input_image = load_image(image_path)
input_image = test_transform({'image': input_image})['image']
input_image = input_image.unsqueeze(0).to(device)

# Make predictions
with torch.no_grad():
    model.eval()
    output = model(input_image)

# Get the predicted class
predicted_class = torch.argmax(output).item()
predicted_label = CIFAR100_LABELS[predicted_class]

print(f'The predicted class is: {predicted_class} - {predicted_label}')